<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Advanced CNN GoogLeNet, ResNet &amp; DenseNet | SHINYUU BLOG</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/7.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-108690246-1','auto');ga('send','pageview');</script><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Advanced CNN GoogLeNet, ResNet &amp; DenseNet</h1><a id="logo" href="/.">SHINYUU BLOG</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Advanced CNN GoogLeNet, ResNet &amp; DenseNet</h1><div class="post-meta">Nov 6, 2017</div><div class="post-content"><p>I hope the previous two articles could be helpful to understand CNNs, and here we introduce a few more recent successful CNN models. Just two years after AlexNet, Google researchers made a big splash with a new network structure – GoogLeNet, the winner of ILSVRC-2014 with a top 5 error rate of 6.67%. Then, researchers at Microsoft Research Asia designed a deeper and structrue-simpler networks – ResNet. It achieved 3.57% error on the ImageNet test set and this result won the 1st place on the ILSVRC-2015 classification task. In 2017, the paper about DenseNet was elected CVPR 2017 best paper. To achieve the same accuracy and precision on the ImageNet classification dataset, compared to ResNet, it is estimisted that DenseNet requires less than half of the parameters and only about half of the computational power.   </p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><p>Here we still use the data set which has been introduced in the previous essay. Same as before, we just need to redefine net functions when we write codes of these new CNNs. Thanks to Gluon and Gluon-tutorials-zh.</p>
<h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><p><img src="http://zh.gluon.ai/_images/googlenet.png" alt="the Architecture of GoogLeNet"><br>As can be seen from the figure above, compared to AlexNet and VGG, GoogLeNet has become more complicated, in which the blue layer reprensents convolutional, the red layer pooling, and the yellow  fully connected layer. </p>
<p>In fact, the backbone of GoogLeNet can be seen as a stack of Inception block modules, each of which has 4 parallel layers as shown below. We could see that the input data gets four results through these four paths of each Inception block, and then we concat together them in the dimension of channel, so we have to gurantee that the 4 outputs are consistent in height and width.<br><img src="http://oyer09cam.bkt.clouddn.com/blog/171105/ie2LAD6dEI.png" alt="Inception Module"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment">##Inception Module</span></div><div class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inception</span><span class="params">(nn.Block)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n1_1, n2_1, n2_3, n3_1, n3_5, n4_1, **kwargs)</span>:</span></div><div class="line">        super(Inception, self).__init__(**kwargs)</div><div class="line">        <span class="comment"># path 1</span></div><div class="line">        self.p1_conv_1 = nn.Conv2D(n1_1, kernel_size=<span class="number">1</span>,activation=<span class="string">'relu'</span>)</div><div class="line">        <span class="comment"># path 2</span></div><div class="line">        self.p2_conv_1 = nn.Conv2D(n2_1, kernel_size=<span class="number">1</span>,activation=<span class="string">'relu'</span>)</div><div class="line">        self.p2_conv_3 = nn.Conv2D(n2_3, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>,activation=<span class="string">'relu'</span>)</div><div class="line">        <span class="comment"># path 3</span></div><div class="line">        self.p3_conv_1 = nn.Conv2D(n3_1, kernel_size=<span class="number">1</span>,activation=<span class="string">'relu'</span>)</div><div class="line">        self.p3_conv_5 = nn.Conv2D(n3_5, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>,activation=<span class="string">'relu'</span>)</div><div class="line">        <span class="comment"># path 4</span></div><div class="line">        self.p4_pool_3 = nn.MaxPool2D(pool_size=<span class="number">3</span>, padding=<span class="number">1</span>,strides=<span class="number">1</span>)</div><div class="line">        self.p4_conv_1 = nn.Conv2D(n4_1, kernel_size=<span class="number">1</span>,activation=<span class="string">'relu'</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        p1 = self.p1_conv_1(x)</div><div class="line">        p2 = self.p2_conv_3(self.p2_conv_1(x))</div><div class="line">        p3 = self.p3_conv_5(self.p3_conv_1(x))</div><div class="line">        p4 = self.p4_conv_1(self.p4_pool_3(x))</div><div class="line">        <span class="keyword">return</span> nd.concat(p1, p2, p3, p4, dim=<span class="number">1</span>)</div></pre></td></tr></table></figure></p>
<p>Adding Inception blocks allows us to use more channels and layers, while controlling the amount of computation and model size within reasonable limits. After the most important module defined, we can build GoogLeNet much more easily. More detailed information could be seen in <a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="external">Going Deeper with Convolutions</a>.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## the Architecture of GoogLeNet</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">GoogLeNet</span><span class="params">(nn.Block)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes, verbose=False, **kwargs)</span>:</span></div><div class="line">        super(GoogLeNet, self).__init__(**kwargs)</div><div class="line">        self.verbose = verbose</div><div class="line">        <span class="comment"># add name_scope on the outer most Sequential</span></div><div class="line">        <span class="keyword">with</span> self.name_scope():</div><div class="line">            <span class="comment"># block 1</span></div><div class="line">            b1 = nn.Sequential()</div><div class="line">            b1.add(</div><div class="line">                nn.Conv2D(<span class="number">64</span>, kernel_size=<span class="number">7</span>, strides=<span class="number">2</span>,padding=<span class="number">3</span>, activation=<span class="string">'relu'</span>),</div><div class="line">                nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)</div><div class="line">            )</div><div class="line">            <span class="comment"># block 2</span></div><div class="line">            b2 = nn.Sequential()</div><div class="line">            b2.add(</div><div class="line">                nn.Conv2D(<span class="number">64</span>, kernel_size=<span class="number">1</span>),</div><div class="line">                nn.Conv2D(<span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</div><div class="line">                nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)</div><div class="line">            )</div><div class="line"></div><div class="line">            <span class="comment"># block 3</span></div><div class="line">            b3 = nn.Sequential()</div><div class="line">            b3.add(</div><div class="line">                Inception(<span class="number">64</span>, <span class="number">96</span>, <span class="number">128</span>, <span class="number">16</span>,<span class="number">32</span>, <span class="number">32</span>),</div><div class="line">                Inception(<span class="number">128</span>, <span class="number">128</span>, <span class="number">192</span>, <span class="number">32</span>, <span class="number">96</span>, <span class="number">64</span>),</div><div class="line">                nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)</div><div class="line">            )</div><div class="line"></div><div class="line">            <span class="comment"># block 4</span></div><div class="line">            b4 = nn.Sequential()</div><div class="line">            b4.add(</div><div class="line">                Inception(<span class="number">192</span>, <span class="number">96</span>, <span class="number">208</span>, <span class="number">16</span>, <span class="number">48</span>, <span class="number">64</span>),</div><div class="line">                Inception(<span class="number">160</span>, <span class="number">112</span>, <span class="number">224</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>),</div><div class="line">                Inception(<span class="number">128</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>),</div><div class="line">                Inception(<span class="number">112</span>, <span class="number">144</span>, <span class="number">288</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">64</span>),</div><div class="line">                Inception(<span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>),</div><div class="line">                nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)</div><div class="line">            )</div><div class="line"></div><div class="line">            <span class="comment"># block 5</span></div><div class="line">            b5 = nn.Sequential()</div><div class="line">            b5.add(</div><div class="line">                Inception(<span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>),</div><div class="line">                Inception(<span class="number">384</span>, <span class="number">192</span>, <span class="number">384</span>, <span class="number">48</span>, <span class="number">128</span>, <span class="number">128</span>),</div><div class="line">                nn.AvgPool2D(pool_size=<span class="number">2</span>)</div><div class="line">            )</div><div class="line">            <span class="comment"># block 6</span></div><div class="line">            b6 = nn.Sequential()</div><div class="line">            b6.add(</div><div class="line">                nn.Flatten(),</div><div class="line">                nn.Dense(num_classes)</div><div class="line">            )</div><div class="line">            <span class="comment"># chain blocks together</span></div><div class="line">            self.net = nn.Sequential()</div><div class="line">            self.net.add(b1, b2, b3, b4, b5, b6)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        out = x</div><div class="line">        <span class="keyword">for</span> i, b <span class="keyword">in</span> enumerate(self.net):</div><div class="line">            out = b(out)</div><div class="line">            <span class="keyword">if</span> self.verbose:</div><div class="line">                print(<span class="string">'Block %d output: %s'</span>%(i+<span class="number">1</span>, out.shape))</div><div class="line">        <span class="keyword">return</span> out</div><div class="line">        </div><div class="line">net = GoogLeNet(<span class="number">10</span>)</div></pre></td></tr></table></figure></p>
<p>Actually, here we could show the changes in the shape of the data between the blocks, and I believe it can help us understand the architecture.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## show data shape changes</span></div><div class="line">net = GoogLeNet(<span class="number">10</span>, verbose=<span class="keyword">True</span>)</div><div class="line">net.initialize()</div><div class="line"></div><div class="line">x = nd.random.uniform(shape=(<span class="number">4</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</div><div class="line">y = net(x)</div></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## Outcome</span></div><div class="line">Block <span class="number">1</span> output: (<span class="number">4</span>, <span class="number">64</span>, <span class="number">55</span>, <span class="number">55</span>)</div><div class="line">Block <span class="number">2</span> output: (<span class="number">4</span>, <span class="number">192</span>, <span class="number">27</span>, <span class="number">27</span>)</div><div class="line">Block <span class="number">3</span> output: (<span class="number">4</span>, <span class="number">480</span>, <span class="number">13</span>, <span class="number">13</span>)</div><div class="line">Block <span class="number">4</span> output: (<span class="number">4</span>, <span class="number">832</span>, <span class="number">6</span>, <span class="number">6</span>)</div><div class="line">Block <span class="number">5</span> output: (<span class="number">4</span>, <span class="number">1024</span>, <span class="number">3</span>, <span class="number">3</span>)</div><div class="line">Block <span class="number">6</span> output: (<span class="number">4</span>, <span class="number">10</span>)</div></pre></td></tr></table></figure>
<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>ResNet effectively solves the problem of difficult training of deep convolutional neural networks. The gradients usually become smaller and smaller during the backpropagation, thus the amount of weights updating of the front layers also becomes smaller, leading the training of layers far from the loss function slower and slower. Let’s see how ResNet tackle it.<br><img src="http://zh.gluon.ai/_images/residual.svg" alt="the core idea of ResNet"><br>The picture above (Source: Gluon-tutorials-zh) shows a skip connection. The bottom layer’s output not only becomes the input of  the middle layer, but also add with the middle layer’s output to reach top. In this way, when the gradient is reversed, the uppermost gradient can be directly skipped to the lowest layer, so as to avoid the situation where the lowest layer gradient is too small.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## Residual Block</span></div><div class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Residual</span><span class="params">(nn.Block)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, channels, same_shape=True, **kwargs)</span>:</span></div><div class="line">        super(Residual, self).__init__(**kwargs)</div><div class="line">        self.same_shape = same_shape</div><div class="line">        strides = <span class="number">1</span> <span class="keyword">if</span> same_shape <span class="keyword">else</span> <span class="number">2</span></div><div class="line">        self.conv1 = nn.Conv2D(channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>,strides=strides)</div><div class="line">        self.bn1 = nn.BatchNorm()</div><div class="line">        self.conv2 = nn.Conv2D(channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</div><div class="line">        self.bn2 = nn.BatchNorm()</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> same_shape:</div><div class="line">            self.conv3 = nn.Conv2D(channels, kernel_size=<span class="number">1</span>,strides=strides)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        out = nd.relu(self.bn1(self.conv1(x)))</div><div class="line">        out = self.bn2(self.conv2(out))</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.same_shape:</div><div class="line">            x = self.conv3(x)</div><div class="line">        <span class="keyword">return</span> nd.relu(out + x)</div></pre></td></tr></table></figure></p>
<p>From the code above, we know that ResNet all uses 3 × 3 kernel, same as VGG, and adds batch normalization layers after convolutional  layers to speed training. If we select same_shape=True, the output and input are in the same height and width, otherwise output size will be halved.<br><img src="http://oyer09cam.bkt.clouddn.com/blog/171106/3b576liAmi.png" alt="the Architecture of ResNet"><br>The main part of ResNet concatenates multiple Residual blocks. Below we define the ResNet 18. At the same time we can notice that ResNet does not use Dropout. Read <a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">Deep Residual Learning for Image Recognition</a> to find more useful information. And if you have more interests in that, try <a href="https://arxiv.org/abs/1603.05027" target="_blank" rel="external">Identity Mappings in Deep Residual Networks</a>, in that paper, they discussed the case of changing  Conv-&gt;BN-&gt;Relu inside of the Residual block to BN-&gt;Relu-&gt;Conv.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## the Architecture of ResNet</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Block)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes, verbose=False, **kwargs)</span>:</span></div><div class="line">        super(ResNet, self).__init__(**kwargs)</div><div class="line">        self.verbose = verbose</div><div class="line">        <span class="keyword">with</span> self.name_scope():</div><div class="line">            <span class="comment"># block 1</span></div><div class="line">            b1 = nn.Conv2D(<span class="number">64</span>, kernel_size=<span class="number">7</span>, strides=<span class="number">2</span>)</div><div class="line">            <span class="comment"># block 2</span></div><div class="line">            b2 = nn.Sequential()</div><div class="line">            b2.add(</div><div class="line">                nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>),</div><div class="line">                Residual(<span class="number">64</span>),</div><div class="line">                Residual(<span class="number">64</span>)</div><div class="line">            )</div><div class="line">            <span class="comment"># block 3</span></div><div class="line">            b3 = nn.Sequential()</div><div class="line">            b3.add(</div><div class="line">                Residual(<span class="number">128</span>, same_shape=<span class="keyword">False</span>),</div><div class="line">                Residual(<span class="number">128</span>)</div><div class="line">            )</div><div class="line">            <span class="comment"># block 4</span></div><div class="line">            b4 = nn.Sequential()</div><div class="line">            b4.add(</div><div class="line">                Residual(<span class="number">256</span>, same_shape=<span class="keyword">False</span>),</div><div class="line">                Residual(<span class="number">256</span>)</div><div class="line">            )</div><div class="line">            <span class="comment"># block 5</span></div><div class="line">            b5 = nn.Sequential()</div><div class="line">            b5.add(</div><div class="line">                Residual(<span class="number">512</span>, same_shape=<span class="keyword">False</span>),</div><div class="line">                Residual(<span class="number">512</span>)</div><div class="line">            )</div><div class="line">            <span class="comment"># block 6</span></div><div class="line">            b6 = nn.Sequential()</div><div class="line">            b6.add(</div><div class="line">                nn.AvgPool2D(pool_size=<span class="number">3</span>),</div><div class="line">                nn.Dense(num_classes)</div><div class="line">            )</div><div class="line">            <span class="comment"># chain all blocks together</span></div><div class="line">            self.net = nn.Sequential()</div><div class="line">            self.net.add(b1, b2, b3, b4, b5, b6)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        out = x</div><div class="line">        <span class="keyword">for</span> i, b <span class="keyword">in</span> enumerate(self.net):</div><div class="line">            out = b(out)</div><div class="line">            <span class="keyword">if</span> self.verbose:</div><div class="line">                print(<span class="string">'Block %d output: %s'</span>%(i+<span class="number">1</span>, out.shape))</div><div class="line">        <span class="keyword">return</span> out</div><div class="line">        </div><div class="line">net = ResNet(<span class="number">10</span>, verbose=<span class="keyword">True</span>)</div></pre></td></tr></table></figure></p>
<p>Here we still show the changes in the shape of the data between the blocks, and I believe it can help us understand the architecture.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## show data shape changes</span></div><div class="line">net = ResNet(<span class="number">10</span>, verbose=<span class="keyword">True</span>)</div><div class="line">net.initialize()</div><div class="line"></div><div class="line">x = nd.random.uniform(shape=(<span class="number">4</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</div><div class="line">y = net(x)</div></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## Outcome</span></div><div class="line">Block <span class="number">1</span> output: (<span class="number">4</span>, <span class="number">64</span>, <span class="number">109</span>, <span class="number">109</span>)</div><div class="line">Block <span class="number">2</span> output: (<span class="number">4</span>, <span class="number">64</span>, <span class="number">54</span>, <span class="number">54</span>)</div><div class="line">Block <span class="number">3</span> output: (<span class="number">4</span>, <span class="number">128</span>, <span class="number">27</span>, <span class="number">27</span>)</div><div class="line">Block <span class="number">4</span> output: (<span class="number">4</span>, <span class="number">256</span>, <span class="number">14</span>, <span class="number">14</span>)</div><div class="line">Block <span class="number">5</span> output: (<span class="number">4</span>, <span class="number">512</span>, <span class="number">7</span>, <span class="number">7</span>)</div><div class="line">Block <span class="number">6</span> output: (<span class="number">4</span>, <span class="number">10</span>)</div></pre></td></tr></table></figure>
<h2 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h2><p>If you really inquire into the core idea of GoogLeNet and ResNet, I think you will get its way easily in understanding DenseNet. The following picture (Source: Gluon-tutorials-zh) shows us how DenseNet is designed. Does it feel familiar? Actually, it seems to learn the residual structure and concat idea form ResNet and GoogLeNet respectively.<br><img src="http://zh.gluon.ai/_images/densenet.svg" alt="the core idea of DenseNet"><br>Let’s define a dense block first. Convolution blocks for DenseNet use ResNet’s improved version BN-&gt; Relu-&gt; Conv. The number of output channels per convolutional is called growth_rate $\gamma$, and if we give the layer number $n$ as well as input channel $m$, then the final output channel would be $m+n*\gamma$ .<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## Dense Block</span></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</div><div class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_block</span><span class="params">(channels)</span>:</span></div><div class="line">    out = nn.Sequential()</div><div class="line">    out.add(</div><div class="line">        nn.BatchNorm(),</div><div class="line">        nn.Activation(<span class="string">'relu'</span>),</div><div class="line">        nn.Conv2D(channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</div><div class="line">    )</div><div class="line">    <span class="keyword">return</span> out</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseBlock</span><span class="params">(nn.Block)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layers, growth_rate, **kwargs)</span>:</span></div><div class="line">        super(DenseBlock, self).__init__(**kwargs)</div><div class="line">        self.net = nn.Sequential()</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(layers):</div><div class="line">            self.net.add(conv_block(growth_rate))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.net:</div><div class="line">            out = layer(x)</div><div class="line">            x = nd.concat(x, out, dim=<span class="number">1</span>)</div><div class="line">        <span class="keyword">return</span> x</div></pre></td></tr></table></figure></p>
<p>Next let’s define a transition block. From the above we know that the number of channels of output will be larger and larger, so we need some way to control it. In order to control the complexity of the model, we introduce a transition block that not only uses AvgPooling to halve the input height and width but also uses 1 × 1 kernel to change the number of channels.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## Transition Block</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">transition_block</span><span class="params">(channels)</span>:</span></div><div class="line">    out = nn.Sequential()</div><div class="line">    out.add(</div><div class="line">        nn.BatchNorm(),</div><div class="line">        nn.Activation(<span class="string">'relu'</span>),</div><div class="line">        nn.Conv2D(channels, kernel_size=<span class="number">1</span>),</div><div class="line">        nn.AvgPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>)</div><div class="line">    )</div><div class="line">    <span class="keyword">return</span> out</div></pre></td></tr></table></figure></p>
<p><img src="http://oyer09cam.bkt.clouddn.com/blog/171106/hb1a5AKFIa.png" alt="the Architecture of a kind of DenseNet with Growth_rate 3 "><br>We use the same growth_rate in building the network, and change the number of layers in each dense block, then use transition block to smaller the size and lower the number of channels behind each dense block. This is the main structure of DenseNet. See <a href="https://arxiv.org/pdf/1608.06993.pdf" target="_blank" rel="external">Densely Connected Convolutional Networks</a> to understand more.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## the Architecture of DenseNet</span></div><div class="line">init_channels = <span class="number">64</span></div><div class="line">growth_rate = <span class="number">32</span></div><div class="line">block_layers = [<span class="number">6</span>, <span class="number">12</span>, <span class="number">24</span>, <span class="number">16</span>]</div><div class="line">num_classes = <span class="number">10</span></div><div class="line"></div><div class="line"></div><div class="line">net = nn.Sequential()</div><div class="line"><span class="keyword">with</span> net.name_scope():</div><div class="line">    <span class="comment"># first block</span></div><div class="line">    net.add(</div><div class="line">        nn.Conv2D(init_channels, kernel_size=<span class="number">7</span>,strides=<span class="number">2</span>, padding=<span class="number">3</span>),</div><div class="line">        nn.BatchNorm(),</div><div class="line">        nn.Activation(<span class="string">'relu'</span>),</div><div class="line">        nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="number">1</span>)</div><div class="line">    )</div><div class="line">    <span class="comment"># dense blocks</span></div><div class="line">    channels = init_channels</div><div class="line">    <span class="keyword">for</span> i, layers <span class="keyword">in</span> enumerate(block_layers):</div><div class="line">        net.add(DenseBlock(layers, growth_rate))</div><div class="line">        channels += layers * growth_rate</div><div class="line">        <span class="keyword">if</span> i != len(block_layers)<span class="number">-1</span>:</div><div class="line">            net.add(transition_block(channels//<span class="number">2</span>))</div><div class="line">    <span class="comment"># last block</span></div><div class="line">    net.add(</div><div class="line">        nn.BatchNorm(),</div><div class="line">        nn.Activation(<span class="string">'relu'</span>),</div><div class="line">        nn.AvgPool2D(pool_size=<span class="number">1</span>),</div><div class="line">        nn.Flatten(),</div><div class="line">        nn.Dense(num_classes)</div><div class="line">    )</div></pre></td></tr></table></figure></p>
<p>Here we still show the changes in the shape of the data between the blocks, and I believe it can help us understand the architecture.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## show data shape changes</span></div><div class="line">net1 = DenseBlock(<span class="number">2</span>, <span class="number">10</span>)</div><div class="line">net1.initialize()</div><div class="line">net2 = transition_block(<span class="number">5</span>)</div><div class="line">net2.initialize()</div><div class="line"></div><div class="line">x = nd.random.uniform(shape=(<span class="number">4</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</div><div class="line">y = net1(x)</div><div class="line">z= net2(y)</div><div class="line">print(y.shape,z.shape)</div></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## Outcome</span></div><div class="line">(<span class="number">4</span>, <span class="number">23</span>, <span class="number">224</span>, <span class="number">224</span>) (<span class="number">4</span>, <span class="number">5</span>, <span class="number">112</span>, <span class="number">112</span>)</div></pre></td></tr></table></figure>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1]: <em>Mu Li</em>. Gluon-tutorials-zh.<br>[2]: <em>Christian Szegedy,etc</em>. Going Deeper with Convolutions.<br>[3]: <em>Kaiming He,etc</em>. Identity Mappings in Deep Residual Networks.<br>[4]: <em>Gao Huang,etc</em>. Densely Connected Convolutional Networks.</p>
</div><div class="tags"><a href="/tags/Python/">Python</a><a href="/tags/NeuralNetwork/">NeuralNetwork</a></div><div class="post-nav"><a href="/AlexnetVgg/" class="next">Advanced CNN AlexNet &amp; VGG</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="search" name="word" maxlength="20" placeholder="Search"/><input type="hidden" name="si" value="http://yoursite.com"/><input name="tn" type="hidden" value="bds"/><input name="cl" type="hidden" value="3"/><input name="ct" type="hidden" value="2097152"/><input name="s" type="hidden" value="on"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/NeuralNetwork/" style="font-size: 15px;">NeuralNetwork</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/GooglenetResnet/">Advanced CNN GoogLeNet, ResNet & DenseNet</a></li><li class="post-list-item"><a class="post-list-link" href="/AlexnetVgg/">Advanced CNN AlexNet & VGG</a></li><li class="post-list-item"><a class="post-list-link" href="/CNN/">Introduction of CNN</a></li><li class="post-list-item"><a class="post-list-link" href="/MLP/">Multilayer Perceptron</a></li><li class="post-list-item"><a class="post-list-link" href="/PythonBasicstutorial/">Python Basics Tutorial</a></li><li class="post-list-item"><a class="post-list-link" href="/Start/">Start</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="https://sites.google.com/view/lixinyuhomepage" title="Homepage" target="_blank">Homepage</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2017 <a href="/." rel="nofollow">SHINYUU BLOG.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body></html>