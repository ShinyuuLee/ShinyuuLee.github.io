<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Introduction of RNN | SHINYUU BLOG</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/7.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-108690246-1','auto');ga('send','pageview');</script><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Introduction of RNN</h1><a id="logo" href="/.">SHINYUU BLOG</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Introduction of RNN</h1><div class="post-meta">Jan 17, 2018</div><div class="post-content"><p>This essay briefly introduces the Recurrent Neural Network (RNN) which are designed to understand the sequential information by adding memory. Unlike feedforward neural networks which we introduce before, RNNs has a notion of order in time, taking as their input not just the current input example they see, but also what they have perceived previously in time. RNNs hold great promise in recognizing patterns in sequences of data. This makes them applicable to many NLP tasks.</p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><h3 id="Single-layer"><a href="#Single-layer" class="headerlink" title="Single-layer"></a>Single-layer</h3><p>At first we introduce a simple single-layer RNN model. Assume that the input at time step $t$ is $\mathbf{X}_t$,$t=1,2,…T$. We can show the process in formula below.<br>$$\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{hx} + \mathbf{H}_{t-1} \mathbf{W}_{hh}  + \mathbf{b}_h)$$</p>
<p>$$\hat{\mathbf{Y}}_t = \text{softmax}(\mathbf{H}_t \mathbf{W}_{yh}  + \mathbf{b}_y)$$</p>
<p>At time step $t$ the memory information is preserved in the recurrent network’s hidden state  $H_{t}$. The weight matrices are filters that determine the importance of the present input and the past hidden state. Then the sum of hidden state and the weight input is squashed by the function $\phi$ which may be a logistic sigmoid or tanh. </p>
<p><img src="http://zh.gluon.ai/_images/rnn-bptt.svg" alt="Structure of RNN(from zh.gluon.ai)"></p>
<p>In order to visualize the calculations between model variables and parameters, we can draw a calculation graph. In this case, the number of steps $T$ is 3. Next we will show how to calculate the partial derivative of loss function $L$ to weight matrices. Without loss of generality, we omit bias term $b$.</p>
<p>$$\frac{\partial L}{\partial \mathbf{o}_t} =  \frac{\partial \ell (\mathbf{o}_t, y_t)}{T \cdot \partial \mathbf{o}_t} $$</p>
<p>$$\frac{\partial L}{\partial \mathbf{W}_{yh}}<br>= \sum_{t=1}^T \text{prod}(\frac{\partial L}{\partial \mathbf{o}_t}, \frac{\partial \mathbf{o}_t}{\partial \mathbf{W}_{yh}})<br>= \sum_{t=1}^T \frac{\partial L}{\partial \mathbf{o}_t} \mathbf{h}_t^\top<br>$$</p>
<p>$$\frac{\partial L}{\partial \mathbf{h}_T} = \text{prod}(\frac{\partial L}{\partial \mathbf{o}_T}, \frac{\partial \mathbf{o}_T}{\partial \mathbf{h}_T} ) = \mathbf{W}_{yh}^\top \frac{\partial L}{\partial \mathbf{o}_T}<br>$$</p>
<p>$$\frac{\partial L}{\partial \mathbf{h}_t}<br>= \text{prod}(\frac{\partial L}{\partial \mathbf{h}_{t+1}}, \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_t} )<br>+ \text{prod}(\frac{\partial L}{\partial \mathbf{o}_t}, \frac{\partial \mathbf{o}_t}{\partial \mathbf{h}_t} )<br>= \mathbf{W}_{hh}^\top \frac{\partial L}{\partial \mathbf{h}_{t+1}} + \mathbf{W}_{yh}^\top \frac{\partial L}{\partial \mathbf{o}_t}<br>= \sum_{i=t}^T {(\mathbf{W}_{hh}^\top)}^{T-i} \mathbf{W}_{yh}^\top \frac{\partial L}{\partial \mathbf{o}_{T+t-i}}<br>$$</p>
<p>$$\frac{\partial L}{\partial \mathbf{W}_{hx}}<br>= \sum_{t=1}^T \text{prod}(\frac{\partial L}{\partial \mathbf{h}_t}, \frac{\partial \mathbf{h}_t}{\partial \mathbf{W}_{hx}})<br>= \sum_{t=1}^T (\frac{\partial L}{\partial \mathbf{h}_t} \odot \phi^\prime) \mathbf{x}_t^\top<br>$$</p>
<p>$$\frac{\partial L}{\partial \mathbf{W}_{hh}}<br>= \sum_{t=1}^T \text{prod}(\frac{\partial L}{\partial \mathbf{h}_t}, \frac{\partial \mathbf{h}_t}{\partial \mathbf{W}_{hh}})<br>= \sum_{t=1}^T (\frac{\partial L}{\partial \mathbf{h}_t} \odot \phi^\prime ) \mathbf{h}_{t-1}^\top<br>$$</p>
<p>Assume that the learning rate is $\eta$. Taking the random gradient descent algorithm as an example, we can iteratively update the values of model parameters through the following several formulas.</p>
<p>$$\mathbf{W}_{hx} = \mathbf{W}_{hx} - \eta \frac{\partial L}{\partial \mathbf{W}_{hx}}$$</p>
<p>$$\mathbf{W}_{hh} = \mathbf{W}_{hh} - \eta \frac{\partial L}{\partial \mathbf{W}_{hh}}$$</p>
<p>$$\mathbf{W}_{yh} = \mathbf{W}_{yh} - \eta \frac{\partial L}{\partial \mathbf{W}_{yh}}$$</p>
<h3 id="Multi-layer"><a href="#Multi-layer" class="headerlink" title="Multi-layer"></a>Multi-layer</h3><p><img src="http://zh.gluon.ai/_images/deep-rnn.svg" alt="Calculation of Hidden State(from zh.gluon.ai)"><br>Compared to single-layer RNNs, multi-layer ones only change the calculation method of hidden state  $H_{t}$. As shown in the picture above, in a recurrent neural network with $L$ hidden layers, we can in turn use the following formula to get the final $H_{t}^{(L+1)}$.<br>$$f(\mathbf{H}_t^{(l-1)}, \mathbf{H}_{t-1}^{(l)}) = \mathbf{H}_t^{(l)}$$</p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1]: <em>Aston Zhang, Mu Li</em>. Gluon-tutorials-zh.</p>
</div><div class="tags"><a href="/tags/NeuralNetwork/">NeuralNetwork</a></div><div class="post-nav"><a href="/LSTM/" class="pre">Advanced RNN LSTM</a><a href="/KaggleCompetition/" class="next">Experience in a Kaggle Competition</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="search" name="word" maxlength="20" placeholder="Search"/><input type="hidden" name="si" value="http://yoursite.com"/><input name="tn" type="hidden" value="bds"/><input name="cl" type="hidden" value="3"/><input name="ct" type="hidden" value="2097152"/><input name="s" type="hidden" value="on"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/NeuralNetwork/" style="font-size: 15px;">NeuralNetwork</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/PythonPlot/">Plotting with Python</a></li><li class="post-list-item"><a class="post-list-link" href="/LSTM/">Advanced RNN LSTM</a></li><li class="post-list-item"><a class="post-list-link" href="/RNN/">Introduction of RNN</a></li><li class="post-list-item"><a class="post-list-link" href="/KaggleCompetition/">Experience in a Kaggle Competition</a></li><li class="post-list-item"><a class="post-list-link" href="/StyleTransfer/">Style Transfer</a></li><li class="post-list-item"><a class="post-list-link" href="/Linux/">Linux Basic Operation</a></li><li class="post-list-item"><a class="post-list-link" href="/TrickCNN/">Tricks for CNN Coding</a></li><li class="post-list-item"><a class="post-list-link" href="/GooglenetResnet/">Advanced CNN GoogLeNet, ResNet & DenseNet</a></li><li class="post-list-item"><a class="post-list-link" href="/AlexnetVgg/">Advanced CNN AlexNet & VGG</a></li><li class="post-list-item"><a class="post-list-link" href="/CNN/">Introduction of CNN</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="https://sites.google.com/view/lixinyuhomepage" title="Homepage" target="_blank">Homepage</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">SHINYUU BLOG.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>